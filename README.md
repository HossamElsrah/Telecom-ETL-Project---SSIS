Hereâ€™s a full README file written in English and without code:

---

# Telecom ETL Project - SSIS

This project demonstrates the process of performing ETL (Extract, Transform, Load) operations for telecom event data using SQL Server Integration Services (SSIS). The main objective is to extract data from CSV files, transform it by applying necessary business rules and validations, and finally, load the cleaned data into a SQL Server database. The project also handles rejected records and ensures that the data is stored with a proper audit trail.

## Project Overview

A telecom company generates CSV files every 5 minutes containing event data related to customer activity. The data includes customer identifiers, event types, and timestamps. The task is to process these CSV files, apply necessary transformations, and store the processed data in a SQL Server database. Any records that do not meet the required criteria will be rejected and stored separately for further analysis.

This project focuses on:
- Extracting data from CSV files.
- Transforming data by applying validation rules and cleaning operations.
- Loading the processed data into a SQL Server database.
- Storing rejected records in a separate table with error details.
- Logging statistics regarding the number of processed, stored, and rejected records.

## Data Source

The source data is provided in CSV files, each containing the following columns:

| Column Name  | Data Type | Length | Nullable | Sample Value             |
|--------------|-----------|--------|----------|--------------------------|
| ID           | Int       | 4      | No       | 156                      |
| IMSI         | String    | 9      | No       | 310120265                |
| IMEI         | String    | 14     | Yes      | 490154203237518         |
| CELL         | Int       | 3      | No       | 123                      |
| LAC          | Int       | 3      | No       | 12                       |
| EVENT_TYPE   | String    | 1      | Yes      | 1                        |
| EVENT_TS     | Timestamp | -      | No       | 16/1/2020 7:45:43        |

### Transformation Rules

1. **ID**: Directly mapped to the `Transaction_id` field.
2. **IMSI**: If `IMSI` is null, reject the record.
3. **IMEI**: Extract the first 8 characters and the last 6 characters. If the size is less than 15 characters or null, replace with `-99999`.
4. **CELL**: Directly mapped, but reject the record if it is null.
5. **LAC**: Directly mapped, but reject the record if it is null.
6. **EVENT_TYPE**: Directly mapped to the `EVENT_TYPE` field.
7. **EVENT_TS**: Validate the timestamp format to ensure it follows `YYYY-MM-DD HH:MM:SS`. Reject if the timestamp is invalid or null.

### Data Storage

- **Processed Data**: Valid and transformed data is loaded into the appropriate tables in the SQL Server database.
- **Rejected Records**: Any records that fail the transformation rules are stored in a separate table (`Rejected_Records`) along with the error details, such as the specific reason for rejection and the original CSV filename.

### Logging and Statistics

While processing the data, several statistics are tracked:
- Total number of records in the CSV file.
- Number of records successfully loaded into the database.
- Number of records rejected due to validation failures.
- A link between the stored data and the original CSV file for traceability.

## Steps to Execute the ETL Process

1. **Extract**: The data is extracted from the CSV file.
   - The `Flat File Source` component in SSIS is used to read the data from the CSV file.

2. **Transform**: Apply the transformation rules to validate and clean the data.
   - Use the `Data Conversion` and `Derived Column` components to manipulate and clean data as per the transformation rules.
   - The `Conditional Split` component is used to handle rejected records, directing them to the rejection table.

3. **Load**: Load the cleaned data into the SQL Server database.
   - Use the `SQL Server Destination` component in SSIS to load the data into the correct tables in the database.
   
4. **Handle Rejected Records**: Rejected records are stored in a `Rejected_Records` table.
   - This table contains the rejected records, their error messages, and the name of the CSV file they came from.

5. **File Management**: After processing the data, the original CSV file is moved to a different folder to indicate that it has been processed successfully.
   - This is achieved using the `File System Task` component in SSIS.

## Directory Structure

- **ETL_Package**: Contains the SSIS package files for processing the data.
- **Data**: A folder where the CSV files are stored before processing.
- **Logs**: A folder for log files that record the ETL process, including any errors, successes, and statistics.

## How to Set Up and Run the ETL Process

1. Clone this repository to your local machine.
2. Open the SSIS package in Visual Studio or SQL Server Data Tools (SSDT).
3. Configure the database connection strings to point to your SQL Server instance.
4. Modify the paths for the source CSV files and destination folders as needed.
5. Run the SSIS package to execute the ETL process.

## Additional Information

- This ETL process handles data efficiently, processing CSV files every 5 minutes.
- The system tracks rejected records for further analysis and troubleshooting.
- The project can be easily extended to include additional validation or transformation logic.

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

This README file is designed to give a clear overview of your project, its goals, setup instructions, and how the ETL process works. You can adjust or expand it based on any additional features or details you'd like to include.
